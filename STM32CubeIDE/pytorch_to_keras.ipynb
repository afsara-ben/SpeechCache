{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# SINCNET+2xCNN"
      ],
      "metadata": {
        "id": "2OwVCPCi3zEw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keras==2.4.0\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense\n",
        "\n",
        "# Create the Conv1D model\n",
        "model = Sequential()\n",
        "\n",
        "# First Conv1D layer\n",
        "model.add(Conv1D(filters=80, kernel_size=401, strides=80, input_shape=(25000, 1), activation='relu'))\n",
        "\n",
        "# Second Conv1D layer\n",
        "model.add(Conv1D(filters=60, kernel_size=5, strides=1, activation='relu', padding='same'))\n",
        "\n",
        "# Third Conv1D layer\n",
        "model.add(Conv1D(filters=60, kernel_size=5, strides=1, activation='relu', padding='same'))\n",
        "\n",
        "# Flatten layer\n",
        "model.add(Flatten())\n",
        "\n",
        "# Fully connected layer\n",
        "model.add(Dense(units=1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Print the model summary\n",
        "model.summary()\n",
        "\n",
        "# Generate example input data\n",
        "input_data = np.random.random((16, 25000, 1))  # Batch size of 16, input shape (25000, 1)\n",
        "\n",
        "# Train the model with example data\n",
        "target_labels = np.random.randint(2, size=16)  # Example binary labels\n",
        "model.fit(input_data, target_labels, epochs=5, batch_size=16)\n"
      ],
      "metadata": {
        "id": "Ke3VEFvpxr_S",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e51f3ce9-c0ab-4a3f-87a1-07dc37f7e62f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting keras==2.4.0\n",
            "  Downloading Keras-2.4.0-py2.py3-none-any.whl (170 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m170.2/170.2 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tensorflow>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from keras==2.4.0) (2.12.0)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.10/dist-packages (from keras==2.4.0) (1.23.5)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.10/dist-packages (from keras==2.4.0) (1.10.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from keras==2.4.0) (6.0.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from keras==2.4.0) (3.9.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.2.0->keras==2.4.0) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.2.0->keras==2.4.0) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.2.0->keras==2.4.0) (2.0.7)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.2.0->keras==2.4.0) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.2.0->keras==2.4.0) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.2.0->keras==2.4.0) (1.57.0)\n",
            "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.2.0->keras==2.4.0) (0.4.14)\n",
            "INFO: pip is looking at multiple versions of tensorflow to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting tensorflow>=2.2.0 (from keras==2.4.0)\n",
            "  Downloading tensorflow-2.13.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (524.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m524.1/524.1 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting flatbuffers>=23.1.21 (from tensorflow>=2.2.0->keras==2.4.0)\n",
            "  Downloading flatbuffers-23.5.26-py2.py3-none-any.whl (26 kB)\n",
            "Collecting tensorflow>=2.2.0 (from keras==2.4.0)\n",
            "  Downloading tensorflow-2.12.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (585.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m585.9/585.9 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading tensorflow-2.11.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (588.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m588.3/588.3 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading tensorflow-2.11.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (588.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m588.3/588.3 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading tensorflow-2.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (578.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m578.1/578.1 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading tensorflow-2.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (578.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m578.0/578.0 MB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading tensorflow-2.9.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (511.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m511.8/511.8 MB\u001b[0m \u001b[31m956.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting flatbuffers<2,>=1.12 (from tensorflow>=2.2.0->keras==2.4.0)\n",
            "  Downloading flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n",
            "INFO: pip is looking at multiple versions of tensorflow to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting tensorflow>=2.2.0 (from keras==2.4.0)\n",
            "  Downloading tensorflow-2.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (511.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m511.8/511.8 MB\u001b[0m \u001b[31m975.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading tensorflow-2.9.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (511.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m511.7/511.7 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading tensorflow-2.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (511.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m511.7/511.7 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading tensorflow-2.8.4-cp310-cp310-manylinux2010_x86_64.whl (498.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m498.1/498.1 MB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting keras-preprocessing>=1.1.1 (from tensorflow>=2.2.0->keras==2.4.0)\n",
            "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.2.0->keras==2.4.0) (16.0.6)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.2.0->keras==2.4.0) (3.3.0)\n",
            "Collecting protobuf<3.20,>=3.9.2 (from tensorflow>=2.2.0->keras==2.4.0)\n",
            "  Downloading protobuf-3.19.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m67.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.2.0->keras==2.4.0) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.2.0->keras==2.4.0) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.2.0->keras==2.4.0) (2.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.2.0->keras==2.4.0) (4.7.1)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.2.0->keras==2.4.0) (1.14.1)\n",
            "Collecting tensorboard<2.9,>=2.8 (from tensorflow>=2.2.0->keras==2.4.0)\n",
            "  Downloading tensorboard-2.8.0-py3-none-any.whl (5.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m86.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorflow-estimator<2.9,>=2.8 (from tensorflow>=2.2.0->keras==2.4.0)\n",
            "  Downloading tensorflow_estimator-2.8.0-py2.py3-none-any.whl (462 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m462.3/462.3 kB\u001b[0m \u001b[31m37.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorflow>=2.2.0 (from keras==2.4.0)\n",
            "  Downloading tensorflow-2.8.3-cp310-cp310-manylinux2010_x86_64.whl (498.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m498.5/498.5 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hINFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "  Downloading tensorflow-2.8.2-cp310-cp310-manylinux2010_x86_64.whl (498.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m498.0/498.0 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading tensorflow-2.8.1-cp310-cp310-manylinux2010_x86_64.whl (498.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m498.0/498.0 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.2.0->keras==2.4.0) (3.20.3)\n",
            "  Downloading tensorflow-2.8.0-cp310-cp310-manylinux2010_x86_64.whl (497.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m497.6/497.6 MB\u001b[0m \u001b[31m898.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0mModel: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d (Conv1D)             (None, 308, 80)           32160     \n",
            "                                                                 \n",
            " conv1d_1 (Conv1D)           (None, 308, 60)           24060     \n",
            "                                                                 \n",
            " conv1d_2 (Conv1D)           (None, 308, 60)           18060     \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 18480)             0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1)                 18481     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 92,761\n",
            "Trainable params: 92,761\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:Internal Python error in the inspect module.\n",
            "Below is the traceback from this internal error.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-11-8a538a83b790>\", line 29, in <cell line: 29>\n",
            "    t\n",
            "NameError: name 't' is not defined\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n",
            "    stb = value._render_traceback_()\n",
            "AttributeError: 'NameError' object has no attribute '_render_traceback_'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1101, in get_records\n",
            "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 248, in wrapped\n",
            "    return f(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n",
            "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
            "  File \"/usr/lib/python3.10/inspect.py\", line 1662, in getinnerframes\n",
            "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
            "  File \"/usr/lib/python3.10/inspect.py\", line 1620, in getframeinfo\n",
            "    filename = getsourcefile(frame) or getfile(frame)\n",
            "  File \"/usr/lib/python3.10/inspect.py\", line 829, in getsourcefile\n",
            "    module = getmodule(object, filename)\n",
            "  File \"/usr/lib/python3.10/inspect.py\", line 878, in getmodule\n",
            "    os.path.realpath(f)] = module.__name__\n",
            "  File \"/usr/lib/python3.10/posixpath.py\", line 396, in realpath\n",
            "    path, ok = _joinrealpath(filename[:0], filename, strict, {})\n",
            "  File \"/usr/lib/python3.10/posixpath.py\", line 416, in _joinrealpath\n",
            "    name, _, rest = rest.partition(sep)\n",
            "KeyboardInterrupt\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-8a538a83b790>\u001b[0m in \u001b[0;36m<cell line: 29>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;31m# Generate example input data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 't' is not defined",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2098\u001b[0m                         \u001b[0;31m# in the engines. This should return a list of strings.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2099\u001b[0;31m                         \u001b[0mstb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render_traceback_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2100\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'NameError' object has no attribute '_render_traceback_'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2099\u001b[0m                         \u001b[0mstb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render_traceback_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2100\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2101\u001b[0;31m                         stb = self.InteractiveTB.structured_traceback(etype,\n\u001b[0m\u001b[1;32m   2102\u001b[0m                                             value, tb, tb_offset=tb_offset)\n\u001b[1;32m   2103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1365\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1366\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1367\u001b[0;31m         return FormattedTB.structured_traceback(\n\u001b[0m\u001b[1;32m   1368\u001b[0m             self, etype, value, tb, tb_offset, number_of_lines_of_context)\n\u001b[1;32m   1369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1265\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose_modes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1266\u001b[0m             \u001b[0;31m# Verbose modes need a full traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1267\u001b[0;31m             return VerboseTB.structured_traceback(\n\u001b[0m\u001b[1;32m   1268\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb_offset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_of_lines_of_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1269\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1122\u001b[0m         \u001b[0;34m\"\"\"Return a nice text document describing the traceback.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1124\u001b[0;31m         formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n\u001b[0m\u001b[1;32m   1125\u001b[0m                                                                tb_offset)\n\u001b[1;32m   1126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mformat_exception_as_a_whole\u001b[0;34m(self, etype, evalue, etb, number_of_lines_of_context, tb_offset)\u001b[0m\n\u001b[1;32m   1080\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1081\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1082\u001b[0;31m         \u001b[0mlast_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_recursion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_etype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1083\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1084\u001b[0m         \u001b[0mframes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_records\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mfind_recursion\u001b[0;34m(etype, value, records)\u001b[0m\n\u001b[1;32m    380\u001b[0m     \u001b[0;31m# first frame (from in to out) that looks different.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_recursion_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m     \u001b[0;31m# Select filename, lineno, func_name to track frames with\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the trained model to a file\n",
        "model.save(\"conv1d_3_model.h5\")\n",
        "\n",
        "print(\"Model saved successfully!\")"
      ],
      "metadata": {
        "id": "DPYrnLuSxwT6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "loaded_model = load_model(\"conv1d_model.h5\")"
      ],
      "metadata": {
        "id": "P5J8YeucunKs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# simple RNN"
      ],
      "metadata": {
        "id": "10m9vGSk4R86"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import SimpleRNN, Dense\n",
        "\n",
        "# Generate some example data\n",
        "num_samples = 100\n",
        "num_timesteps = 50\n",
        "num_features = 1  # Number of input features per time step\n",
        "\n",
        "# Create random input data (num_samples, num_timesteps, num_features)\n",
        "input_data = np.random.rand(num_samples, num_timesteps, num_features)\n",
        "\n",
        "# Create a simple RNN model\n",
        "model = Sequential()\n",
        "\n",
        "# Add a SimpleRNN layer with 64 units\n",
        "model.add(SimpleRNN(units=64, activation='relu', input_shape=(num_timesteps, num_features)))\n",
        "\n",
        "# Add a Dense layer for classification\n",
        "model.add(Dense(units=1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Print the model summary\n",
        "model.summary()\n",
        "\n",
        "# Generate example target labels\n",
        "target_labels = np.random.randint(2, size=num_samples)  # Example binary labels\n",
        "\n",
        "# Train the model\n",
        "model.fit(input_data, target_labels, epochs=10, batch_size=16)\n",
        "\n",
        "# Save the trained model to a file\n",
        "model.save(\"rnn_model.h5\")\n",
        "\n",
        "print(\"Model saved successfully!\")"
      ],
      "metadata": {
        "id": "jUFwLtNa4O81"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e2FpK0ja4TFl",
        "outputId": "a5ed012a-ca59-4536-ee94-88ccfb2a9265"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GRU"
      ],
      "metadata": {
        "id": "xul-WxRk4eyq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Bidirectional, GRU, Dense\n",
        "\n",
        "# Generate some example data\n",
        "num_samples = 100\n",
        "input_length = 50\n",
        "num_features = 10  # Number of input features per time step\n",
        "\n",
        "# Create random input data (num_samples, input_length, num_features)\n",
        "input_data = np.random.rand(num_samples, input_length, num_features)\n",
        "\n",
        "# Create the Bidirectional GRU model\n",
        "model = Sequential()\n",
        "\n",
        "# Add a Bidirectional GRU layer with 64 units\n",
        "model.add(Bidirectional(GRU(units=64, return_sequences=True), input_shape=(input_length, num_features)))\n",
        "\n",
        "# Add a Dense output layer with 1 unit and sigmoid activation\n",
        "model.add(Dense(units=1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Print the model summary\n",
        "model.summary()\n",
        "\n",
        "# Generate example target labels\n",
        "# Generate example target labels\n",
        "target_labels = np.random.randint(2, size=(num_samples, 1))  # Example binary labels\n",
        "\n",
        "# Train the model\n",
        "model.fit(input_data, target_labels, epochs=10, batch_size=16)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "brWvSIY35d7t",
        "outputId": "12cf6cb0-536a-4ae0-cc8b-069d2610ad58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " bidirectional_7 (Bidirectio  (None, 50, 128)          29184     \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 50, 1)             129       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 29,313\n",
            "Trainable params: 29,313\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "7/7 [==============================] - 5s 97ms/step - loss: 0.6962 - accuracy: 0.5144\n",
            "Epoch 2/10\n",
            "7/7 [==============================] - 0s 48ms/step - loss: 0.6859 - accuracy: 0.5600\n",
            "Epoch 3/10\n",
            "7/7 [==============================] - 0s 47ms/step - loss: 0.6856 - accuracy: 0.5600\n",
            "Epoch 4/10\n",
            "7/7 [==============================] - 0s 51ms/step - loss: 0.6868 - accuracy: 0.5594\n",
            "Epoch 5/10\n",
            "7/7 [==============================] - 0s 44ms/step - loss: 0.6865 - accuracy: 0.5598\n",
            "Epoch 6/10\n",
            "7/7 [==============================] - 0s 46ms/step - loss: 0.6821 - accuracy: 0.5598\n",
            "Epoch 7/10\n",
            "7/7 [==============================] - 0s 46ms/step - loss: 0.6858 - accuracy: 0.5624\n",
            "Epoch 8/10\n",
            "7/7 [==============================] - 0s 43ms/step - loss: 0.6903 - accuracy: 0.5458\n",
            "Epoch 9/10\n",
            "7/7 [==============================] - 0s 47ms/step - loss: 0.6894 - accuracy: 0.5604\n",
            "Epoch 10/10\n",
            "7/7 [==============================] - 0s 44ms/step - loss: 0.6853 - accuracy: 0.5600\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x78b1aaf8e260>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the trained model to a file\n",
        "model.save(\"gru_model.h5\")\n",
        "\n",
        "print(\"Model saved successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t5P50O0c5giN",
        "outputId": "6c1e39ed-c7a8-43b9-8f4f-ba6fc405eede"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2xGRU"
      ],
      "metadata": {
        "id": "g_mtdxvN4jrg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Bidirectional, GRU, Dense, AveragePooling1D, Dropout\n",
        "import tensorflow as tf\n",
        "import tensorflow_model_optimization as tfmot\n",
        "\n",
        "# Generate some example data\n",
        "num_samples = 1\n",
        "input_length = 265\n",
        "num_features = 60  # Number of input features per time step\n",
        "\n",
        "# Create random input data (num_samples, input_length, num_features)\n",
        "input_data = np.random.rand(num_samples, input_length, num_features)\n",
        "\n",
        "# Parameters\n",
        "num_hidden = (128, 128)\n",
        "downsample_len = (2, 2)\n",
        "downsample_type = ('avg', 'avg')\n",
        "phone_rnn_drop = (0.5, 0.5)\n",
        "phone_rnn_bidirectional = True\n",
        "pooling_dropout = 0.5  # Dropout rate before each AveragePooling1D\n",
        "\n",
        "# Create the Bidirectional GRU model\n",
        "model = Sequential()\n",
        "\n",
        "for hidden_units, ds_len, ds_type, drop_rate in zip(num_hidden, downsample_len, downsample_type, phone_rnn_drop):\n",
        "    # Bidirectional GRU layer\n",
        "    model.add(Bidirectional(GRU(units=hidden_units, return_sequences=True, dropout=drop_rate), input_shape=(input_length, num_features)))\n",
        "\n",
        "    # Dropout layer before AveragePooling1D\n",
        "    model.add(Dropout(pooling_dropout))\n",
        "\n",
        "    # AveragePooling1D layer for downsampling\n",
        "    if ds_type == 'avg':\n",
        "        model.add(AveragePooling1D(pool_size=ds_len))\n",
        "\n",
        "# Add a Dense output layer with 1 unit and sigmoid activation\n",
        "model.add(Dense(units=1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Print the model summary\n",
        "model.summary()\n",
        "\n",
        "# Generate example target labels\n",
        "target_labels = np.random.randint(2, size=(num_samples, 1))  # Example binary labels with shape (None, 1)\n",
        "\n",
        "# Train the model\n",
        "model.fit(input_data, target_labels, epochs=10, batch_size=16)"
      ],
      "metadata": {
        "id": "CNLJ3Plz-Q76"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the trained model to a file\n",
        "model.save(\"gru_2_model.h5\")\n",
        "\n",
        "print(\"Model saved successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dgdxYjul-eQk",
        "outputId": "44c26c68-ec1e-4bee-832a-1c5b21b7405d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Bidirectional, GRU, Dense, AveragePooling1D, Dropout\n",
        "import tensorflow as tf\n",
        "import tensorflow_model_optimization as tfmot\n",
        "\n",
        "# Generate some example data\n",
        "num_samples = 1\n",
        "input_length = 10\n",
        "num_features = 60  # Number of input features per time step\n",
        "\n",
        "# Create random input data (num_samples, input_length, num_features)\n",
        "input_data = np.random.rand(num_samples, input_length, num_features)\n",
        "\n",
        "# Parameters\n",
        "num_hidden = (128, 128)\n",
        "downsample_len = (2, 2)\n",
        "downsample_type = ('avg', 'avg')\n",
        "phone_rnn_drop = (0.5, 0.5)\n",
        "# num_hidden = (128)\n",
        "# downsample_len = (2)\n",
        "# downsample_type = ('avg')\n",
        "# phone_rnn_drop = (0.5)\n",
        "phone_rnn_bidirectional = True\n",
        "pooling_dropout = 0.5  # Dropout rate before each AveragePooling1D\n",
        "\n",
        "# Create the Bidirectional GRU model\n",
        "model = Sequential()\n",
        "\n",
        "# # Bidirectional GRU layer\n",
        "# model.add(Bidirectional(GRU(units=num_hidden, return_sequences=True, dropout=phone_rnn_drop), input_shape=(input_length, num_features)))\n",
        "\n",
        "# # Dropout layer before AveragePooling1D\n",
        "# model.add(Dropout(pooling_dropout))\n",
        "\n",
        "# # AveragePooling1D layer for downsampling\n",
        "# if downsample_type == 'avg':\n",
        "#    model.add(AveragePooling1D(pool_size=downsample_len))\n",
        "\n",
        "for hidden_units, ds_len, ds_type, drop_rate in zip(num_hidden, downsample_len, downsample_type, phone_rnn_drop):\n",
        "    # Bidirectional GRU layer\n",
        "    model.add(Bidirectional(GRU(units=hidden_units, return_sequences=True, dropout=drop_rate), input_shape=(input_length, num_features)))\n",
        "\n",
        "    # Dropout layer before AveragePooling1D\n",
        "    model.add(Dropout(pooling_dropout))\n",
        "\n",
        "    # AveragePooling1D layer for downsampling\n",
        "    if ds_type == 'avg':\n",
        "        model.add(AveragePooling1D(pool_size=ds_len))\n",
        "\n",
        "\n",
        "# Add a Dense output layer with 1 unit and sigmoid activation\n",
        "model.add(Dense(units=1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Print the model summary\n",
        "model.summary()\n",
        "\n",
        "# Generate example target labels\n",
        "target_labels = np.random.randint(2, size=(num_samples, 1))  # Example binary labels with shape (None, 1)\n",
        "\n",
        "# Train the model\n",
        "model.fit(input_data, target_labels, epochs=10, batch_size=16)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8P7_I6lAV-OY",
        "outputId": "389b307f-5851-4265-ee96-92a749de59ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " bidirectional_1 (Bidirectio  (None, 10, 256)          145920    \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 10, 256)           0         \n",
            "                                                                 \n",
            " average_pooling1d (AverageP  (None, 5, 256)           0         \n",
            " ooling1D)                                                       \n",
            "                                                                 \n",
            " bidirectional_2 (Bidirectio  (None, 5, 256)           296448    \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 5, 256)            0         \n",
            "                                                                 \n",
            " average_pooling1d_1 (Averag  (None, 2, 256)           0         \n",
            " ePooling1D)                                                     \n",
            "                                                                 \n",
            " dense (Dense)               (None, 2, 1)              257       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 442,625\n",
            "Trainable params: 442,625\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "1/1 [==============================] - 17s 17s/step - loss: 0.5474 - accuracy: 1.0000\n",
            "Epoch 2/10\n",
            "1/1 [==============================] - 0s 68ms/step - loss: 0.3483 - accuracy: 1.0000\n",
            "Epoch 3/10\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 0.2067 - accuracy: 1.0000\n",
            "Epoch 4/10\n",
            "1/1 [==============================] - 0s 92ms/step - loss: 0.1309 - accuracy: 1.0000\n",
            "Epoch 5/10\n",
            "1/1 [==============================] - 0s 118ms/step - loss: 0.0188 - accuracy: 1.0000\n",
            "Epoch 6/10\n",
            "1/1 [==============================] - 0s 85ms/step - loss: 0.0328 - accuracy: 1.0000\n",
            "Epoch 7/10\n",
            "1/1 [==============================] - 0s 77ms/step - loss: 0.0173 - accuracy: 1.0000\n",
            "Epoch 8/10\n",
            "1/1 [==============================] - 0s 87ms/step - loss: 0.0060 - accuracy: 1.0000\n",
            "Epoch 9/10\n",
            "1/1 [==============================] - 0s 77ms/step - loss: 0.0066 - accuracy: 1.0000\n",
            "Epoch 10/10\n",
            "1/1 [==============================] - 0s 94ms/step - loss: 0.0037 - accuracy: 1.0000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x78e160d869e0>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the trained model to a file\n",
        "model.save(\"gru_2.h5\")\n",
        "\n",
        "print(\"Model saved successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AD0FjG1uWIw9",
        "outputId": "87f185a3-9bdf-4717-aab5-0f4903c5ca3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Resnet sample code"
      ],
      "metadata": {
        "id": "IvQNKpRa4n65"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w7Exb3HCPwS-"
      },
      "outputs": [],
      "source": [
        "!pip install onnx_tf\n",
        "!pip install tf2onnx\n",
        "!pip install tensorflow\n",
        "!pip install tensorflow_model_optimization\n",
        "\n",
        "# !pip uninstall onnx_tfy\n",
        "# !pip install git+https://github.com/onnx/onnx-tensorflow.git@7d8fa7d88fab469253d75e5e11cf9cdcb90104c4"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import onnx\n",
        "import tensorflow as tf\n",
        "import onnx_tf\n",
        "from torchvision.models import resnet50\n",
        "\n",
        "# Load the PyTorch ResNet50 model\n",
        "pytorch_model = resnet50(pretrained=True)\n",
        "pytorch_model.eval()\n",
        "\n",
        "# Export the PyTorch model to ONNX format\n",
        "input_shape = (1, 3, 224, 224)\n",
        "dummy_input = torch.randn(input_shape)\n",
        "onnx_model_path = 'resnet50.onnx'\n",
        "torch.onnx.export(pytorch_model, dummy_input, onnx_model_path, opset_version=12, verbose=False)\n",
        "\n",
        "# Load the ONNX model\n",
        "onnx_model = onnx.load(onnx_model_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JRbstxVGP1vY",
        "outputId": "ca1f6f13-9d38-436f-86ce-6c8bcc6d9eda"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
            "\n",
            "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
            "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
            "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
            "\n",
            "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
            "\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n",
            "100%|██████████| 97.8M/97.8M [00:00<00:00, 149MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============= Diagnostic Run torch.onnx.export version 2.0.1+cu118 =============\n",
            "verbose: False, log level: Level.ERROR\n",
            "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import onnx\n",
        "\n",
        "onnx_model = onnx.load(onnx_model_path)\n",
        "print(\"Model Inputs: \", [inp.name for inp in onnx_model.graph.input])\n",
        "print(\"Model Outputs: \", [out.name for out in onnx_model.graph.output])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X8Ynr6MyrNn9",
        "outputId": "0426942e-242c-4f94-c8c0-64ae6e51b918"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Inputs:  ['input.1']\n",
            "Model Outputs:  ['495']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import onnx\n",
        "from onnx import helper\n",
        "\n",
        "onnx_model = onnx.load(onnx_model_path)\n",
        "\n",
        "# Define a mapping from old names to new names\n",
        "name_map = {\"input.1\": \"input_1\"}\n",
        "\n",
        "# Initialize a list to hold the new inputs\n",
        "new_inputs = []\n",
        "\n",
        "# Iterate over the inputs and change their names if needed\n",
        "for inp in onnx_model.graph.input:\n",
        "    if inp.name in name_map:\n",
        "        # Create a new ValueInfoProto with the new name\n",
        "        new_inp = helper.make_tensor_value_info(name_map[inp.name],\n",
        "                                                inp.type.tensor_type.elem_type,\n",
        "                                                [dim.dim_value for dim in inp.type.tensor_type.shape.dim])\n",
        "        new_inputs.append(new_inp)\n",
        "    else:\n",
        "        new_inputs.append(inp)\n",
        "\n",
        "# Clear the old inputs and add the new ones\n",
        "onnx_model.graph.ClearField(\"input\")\n",
        "onnx_model.graph.input.extend(new_inputs)\n",
        "\n",
        "# Go through all nodes in the model and replace the old input name with the new one\n",
        "for node in onnx_model.graph.node:\n",
        "    for i, input_name in enumerate(node.input):\n",
        "        if input_name in name_map:\n",
        "            node.input[i] = name_map[input_name]\n",
        "\n",
        "# Save the renamed ONNX model\n",
        "onnx.save(onnx_model, 'resnet50-new.onnx')"
      ],
      "metadata": {
        "id": "xU5eZv-5ri0s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "onnx_model = onnx.load(onnx_model_path)\n",
        "print(\"Model Inputs: \", [inp.name for inp in onnx_model.graph.input])\n",
        "print(\"Model Outputs: \", [out.name for out in onnx_model.graph.output])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "djOLRnYttOg3",
        "outputId": "de54d74c-9d51-44b7-966b-aecfebcd1f49"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Inputs:  ['input_1']\n",
            "Model Outputs:  ['495']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import onnx\n",
        "\n",
        "onnx_model_path = 'resnet50-new.onnx'\n",
        "onnx_model = onnx.load(onnx_model_path)\n",
        "from onnx_tf.backend import prepare\n",
        "\n",
        "tf_model_path = 'resnet50'\n",
        "tf_rep = prepare(onnx_model)\n",
        "tf_rep.export_graph(tf_model_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VOmYq_YdkrLt",
        "outputId": "2cc6068f-034d-47f4-b7c0-497f2a2df62e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as gen_tensor_dict while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "onnx_model.graph.node"
      ],
      "metadata": {
        "id": "rEqviPLRssLk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the TensorFlow model to TensorFlow Lite format\n",
        "converter = tf.compat.v1.lite.TFLiteConverter.from_saved_model(tf_model_path)\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "# Save the TensorFlow Lite model to a file\n",
        "with open('resnet50.tflite', 'wb') as f:\n",
        "    f.write(tflite_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2gdICMBYtmmB",
        "outputId": "1c52d7e4-cdbb-4e14-943b-746964968de6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/convert_saved_model.py:42: load (from tensorflow.python.saved_model.loader_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.saved_model.load` instead.\n",
            "INFO:absl:Using experimental converter: If you encountered a problem please file a bug. You can opt-out by setting experimental_new_converter=False\n"
          ]
        }
      ]
    }
  ]
}